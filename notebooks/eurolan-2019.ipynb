{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# EUROLAN 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Prepare for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 1. Merge corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The `/data/corpus` directory contains two corpora:\n",
    "1. The manually annotated corpus and\n",
    "2. The rest of the training corpus\n",
    "\n",
    "In order to be used by the model, these corpora need to be merged. This can be done with the utility script `utils/merge.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "python3 ./utils/merge.py --manually-annotated /data/corpus/team_annotation.xml --training-corpus /data/corpus/rest_of_corpus.xml --output-corpus /data/corpus/train.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 2. Convert the training corpus into `CoNLL-U` format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The corpus resulted after the merge is in `xml` format but `NLP-Cube` requires `CoNLL-U` format. To convert the corpus into the required format we need to feed it to the converter application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### 2.1. Create directories required by the converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The converter requires an input and output directories. Let's create them in `/tmp/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! mkdir /tmp/xml && mkdir /tmp/conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### 2.2. Move the corpus into the `/tmp/xml` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! mv /data/corpus/train.xml /tmp/xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### 2.3. Invoke the converter app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! java -Dfile.encoding=utf-8 -jar /work/bin/TreeBankAnnotatorToConllU.jar /tmp/xml /tmp/conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's see how the file looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! head /tmp/conllu/train.conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### 2.4. Move the converted corpus back to `/data/corpus` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! mv /tmp/conllu/train.conll /data/corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! python3 /work/NLP-Cube/cube/main.py \\\n",
    "    --train=parser \\\n",
    "    --train-file=/data/corpus/train.conll \\\n",
    "    --dev-file=/data/corpus/train.conll \\\n",
    "    --embeddings /data/cc.ro.300.vec \\\n",
    "    --store /model/1.0/parser \\\n",
    "    --batch-size 1000 \\\n",
    "    --set-mem 8000 \\\n",
    "    --autobatch \\\n",
    "    --patience 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. [NLP-Cube: End-to-End Raw Text Processing With Neural Networks](http://www.aclweb.org/anthology/K18-2017), Boroș, Tiberiu and Dumitrescu, Stefan Daniel and Burtica, Ruxandra, Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, Association for Computational Linguistics. p. 171--179. October 2018\n",
    "\n",
    "```\n",
    "@InProceedings{boro-dumitrescu-burtica:2018:K18-2,\n",
    "  author    = {Boroș, Tiberiu  and  Dumitrescu, Stefan Daniel  and  Burtica, Ruxandra},\n",
    "  title     = {{NLP}-Cube: End-to-End Raw Text Processing With Neural Networks},\n",
    "  booktitle = {Proceedings of the {CoNLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},\n",
    "  month     = {October},\n",
    "  year      = {2018},\n",
    "  address   = {Brussels, Belgium},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {171--179},\n",
    "  abstract  = {We introduce NLP-Cube: an end-to-end Natural Language Processing framework, evaluated in CoNLL's \"Multilingual Parsing from Raw Text to Universal Dependencies 2018\" Shared Task. It performs sentence splitting, tokenization, compound word expansion, lemmatization, tagging and parsing. Based entirely on recurrent neural networks, written in Python, this ready-to-use open source system is freely available on GitHub. For each task we describe and discuss its specific network architecture, closing with an overview on the results obtained in the competition.},\n",
    "  url       = {http://www.aclweb.org/anthology/K18-2017}\n",
    "}\n",
    "```\n",
    "\n",
    "2. [Learning Word Vectors for 157 Languages](https://arxiv.org/abs/1802.06893), E. Grave, P. Bojanowski*, P. Gupta, A. Joulin, T. Mikolov\n",
    "```\n",
    "@inproceedings{grave2018learning,\n",
    "  title={Learning Word Vectors for 157 Languages},\n",
    "  author={Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},\n",
    "  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},\n",
    "  year={2018}\n",
    "}\n",
    "```\n",
    "\n",
    "3. Mărănduc, Cătălina Perez, Cenel-Augusto, 2015. A Romanian Dependency Treebank. In International Journal of Computational Linguistics and Applications, vol. 6, no. 2, issue July-December 2015, p. 25–40\n",
    "```\n",
    "@article{DBLP:journals/ijcla/MaranducP15,\n",
    "  author    = {Catalina Maranduc and\n",
    "               Cenel{-}Augusto Perez},\n",
    "  title     = {A Romanian Dependency Treebank},\n",
    "  journal   = {Int. J. Comput. Linguistics Appl.},\n",
    "  volume    = {6},\n",
    "  number    = {2},\n",
    "  pages     = {83--103},\n",
    "  year      = {2015},\n",
    "  url       = {http://www.ijcla.bahripublications.com/2015-2/IJCLA-2015-2-pp-083-103-A-Romanian-Dependency-Treebank.pdf},\n",
    "  timestamp = {Fri, 07 Apr 2017 20:51:02 +0200},\n",
    "  biburl    = {https://dblp.org/rec/bib/journals/ijcla/MaranducP15},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "name": "eurolan-2019.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
